{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Indonesian SMS Data (Traditional ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary libraries have been imported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tertius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tertius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# --- Standard & Data Handling ---\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Plotting & Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- NLP & Text Processing ---\n",
    "import nltk\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader as api\n",
    "\n",
    "# --- Deep Learning (Original Imports) ---\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# --- Machine Learning (Scikit-learn and LightGBM) ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # Added GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix # Added confusion_matrix\n",
    "import lightgbm as lgb # Added LightGBM\n",
    "\n",
    "# --- Download required NLTK data ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"All necessary libraries have been imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet Date</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Quote Count</th>\n",
       "      <th>Reply Count</th>\n",
       "      <th>Retweet Count</th>\n",
       "      <th>Favorite Count</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>English Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thu Feb 29 11:21:27 +0000 2024</td>\n",
       "      <td>Gk muluk muluk, 100,000 lot saham BBCA aja</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Not too ambitious, just 100,000 lots of BBCA s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thu Feb 29 10:11:05 +0000 2024</td>\n",
       "      <td>BCA Expoversary 2024 menawarkan promo suku bun...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>BCA Expoversary 2024 offers special interest r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thu Feb 29 10:06:04 +0000 2024</td>\n",
       "      <td>[USERNAME] saham bca nya menyusul ya ðŸ™‚</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[USERNAME] BCA shares will follow ðŸ™‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thu Feb 29 07:42:09 +0000 2024</td>\n",
       "      <td>PT Bank BCA Syariah (BCA Syariah) turut memeri...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>PT Bank BCA Syariah (BCA Syariah) also enliven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thu Feb 29 06:06:17 +0000 2024</td>\n",
       "      <td>[USERNAME] Begitu byk saham kamu memilih saham...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[USERNAME] So many stocks you choose those sto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Tweet Date  \\\n",
       "0  Thu Feb 29 11:21:27 +0000 2024   \n",
       "1  Thu Feb 29 10:11:05 +0000 2024   \n",
       "2  Thu Feb 29 10:06:04 +0000 2024   \n",
       "3  Thu Feb 29 07:42:09 +0000 2024   \n",
       "4  Thu Feb 29 06:06:17 +0000 2024   \n",
       "\n",
       "                                            Sentence  Quote Count  \\\n",
       "0         Gk muluk muluk, 100,000 lot saham BBCA aja            0   \n",
       "1  BCA Expoversary 2024 menawarkan promo suku bun...            0   \n",
       "2             [USERNAME] saham bca nya menyusul ya ðŸ™‚            0   \n",
       "3  PT Bank BCA Syariah (BCA Syariah) turut memeri...            0   \n",
       "4  [USERNAME] Begitu byk saham kamu memilih saham...            0   \n",
       "\n",
       "   Reply Count  Retweet Count  Favorite Count Sentiment  \\\n",
       "0            0              0               0  Positive   \n",
       "1            0              0               0   Neutral   \n",
       "2            0              0               0  Positive   \n",
       "3            0              0               0   Neutral   \n",
       "4            0              0               1  Positive   \n",
       "\n",
       "                                 English Translation  \n",
       "0  Not too ambitious, just 100,000 lots of BBCA s...  \n",
       "1  BCA Expoversary 2024 offers special interest r...  \n",
       "2                [USERNAME] BCA shares will follow ðŸ™‚  \n",
       "3  PT Bank BCA Syariah (BCA Syariah) also enliven...  \n",
       "4  [USERNAME] So many stocks you choose those sto...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('IDSMSA.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Date             0\n",
      "Sentence               0\n",
      "Quote Count            0\n",
      "Reply Count            0\n",
      "Retweet Count          0\n",
      "Favorite Count         0\n",
      "Sentiment              0\n",
      "English Translation    0\n",
      "dtype: int64\n",
      "Sentiment\n",
      "Positive    1769\n",
      "Negative     786\n",
      "Neutral      733\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check class distribution\n",
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 2630\n",
      "Test set size: 658\n"
     ]
    }
   ],
   "source": [
    "# Clean and map sentiment labels\n",
    "df.dropna(subset=['Sentiment'], inplace=True)\n",
    "df['Sentiment'] = df['Sentiment'].str.lower().str.strip()\n",
    "label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "df['label'] = df['Sentiment'].map(label_map)\n",
    "df.dropna(subset=['label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['Sentence'], \n",
    "    df['label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "print(f'Train set size: {len(X_train)}')\n",
    "print(f'Test set size: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traditional ML Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Indonesian Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword_remover = stopword_factory.create_stop_word_remover()\n",
    "\n",
    "def preprocess_text(text, use_stopwords=True, use_stemming=True):\n",
    "    text = text.lower()\n",
    "    if use_stopwords:\n",
    "        text = stopword_remover.remove(text)\n",
    "    if use_stemming:\n",
    "        text = stemmer.stem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Advanced Indonesian Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More comprehensive stopword list\n",
    "stopword_list = nltk.corpus.stopwords.words('indonesian')\n",
    "\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS_EMO:\n",
    "        text = text.replace(emot, '_'.join(EMOTICONS_EMO[emot].replace(',', '').split()))\n",
    "    return text\n",
    "\n",
    "def preprocess_text_advanced(text, use_stopwords=True, use_stemming=True):\n",
    "    text = convert_emoticons(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s_]', '', text) # Remove punctuation and special chars\n",
    "    tokens = word_tokenize(text)\n",
    "    if use_stopwords:\n",
    "        tokens = [word for word in tokens if word not in stopword_list]\n",
    "    text = ' '.join(tokens)\n",
    "    if use_stemming:\n",
    "        text = stemmer.stem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. TF-IDF Vectorization and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_options = {\n",
    "    'Stemming + Stopwords (Sastrawi)': lambda x: preprocess_text(x, use_stopwords=True, use_stemming=True),\n",
    "    'Stemming Only (Sastrawi)': lambda x: preprocess_text(x, use_stopwords=False, use_stemming=True),\n",
    "    'Stopwords Only (Sastrawi)': lambda x: preprocess_text(x, use_stopwords=True, use_stemming=False),\n",
    "    'No Preprocessing': lambda x: x,\n",
    "    'Advanced (Stem+Stop+Emoticon)': lambda x: preprocess_text_advanced(x, use_stopwords=True, use_stemming=True),\n",
    "    'Advanced (Stop+Emoticon)': lambda x: preprocess_text_advanced(x, use_stopwords=True, use_stemming=False),\n",
    "    'Advanced (Stem+Emoticon)': lambda x: preprocess_text_advanced(x, use_stopwords=False, use_stemming=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preprocessing: Stemming + Stopwords (Sastrawi) ---\n",
      "================================================================================\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LogisticRegression with Stemming + Stopwords (Sastrawi) ---\n",
      "Best Hyperparameters: {'C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.81      0.85      0.83       354\n",
      "     neutral       0.66      0.54      0.60       147\n",
      "    negative       0.71      0.76      0.73       157\n",
      "\n",
      "    accuracy                           0.76       658\n",
      "   macro avg       0.73      0.72      0.72       658\n",
      "weighted avg       0.75      0.76      0.75       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- MultinomialNB with Stemming + Stopwords (Sastrawi) ---\n",
      "Best Hyperparameters: {'alpha': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.70      0.94      0.80       354\n",
      "     neutral       0.86      0.37      0.51       147\n",
      "    negative       0.76      0.57      0.65       157\n",
      "\n",
      "    accuracy                           0.72       658\n",
      "   macro avg       0.77      0.63      0.66       658\n",
      "weighted avg       0.75      0.72      0.70       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LinearSVC with Stemming + Stopwords (Sastrawi) ---\n",
      "Best Hyperparameters: {'C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.80      0.85      0.83       354\n",
      "     neutral       0.65      0.59      0.62       147\n",
      "    negative       0.75      0.70      0.72       157\n",
      "\n",
      "    accuracy                           0.76       658\n",
      "   macro avg       0.73      0.72      0.72       658\n",
      "weighted avg       0.75      0.76      0.76       658\n",
      "\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10122\n",
      "[LightGBM] [Info] Number of data points in the train set: 2630, number of used features: 477\n",
      "[LightGBM] [Info] Start training from score -0.619854\n",
      "[LightGBM] [Info] Start training from score -1.501419\n",
      "[LightGBM] [Info] Start training from score -1.430608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tertius\\sekolah\\Sem 6\\textmining\\AOL\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LGBMClassifier with Stemming + Stopwords (Sastrawi) ---\n",
      "Best Hyperparameters: {'learning_rate': 0.05, 'n_estimators': 100, 'num_leaves': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.88      0.81       354\n",
      "     neutral       0.67      0.45      0.54       147\n",
      "    negative       0.72      0.70      0.71       157\n",
      "\n",
      "    accuracy                           0.74       658\n",
      "   macro avg       0.72      0.68      0.69       658\n",
      "weighted avg       0.73      0.74      0.73       658\n",
      "\n",
      "--- Preprocessing: Stemming Only (Sastrawi) ---\n",
      "================================================================================\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LogisticRegression with Stemming Only (Sastrawi) ---\n",
      "Best Hyperparameters: {'C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.83      0.85      0.84       354\n",
      "     neutral       0.69      0.63      0.65       147\n",
      "    negative       0.73      0.74      0.73       157\n",
      "\n",
      "    accuracy                           0.78       658\n",
      "   macro avg       0.75      0.74      0.74       658\n",
      "weighted avg       0.77      0.78      0.77       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- MultinomialNB with Stemming Only (Sastrawi) ---\n",
      "Best Hyperparameters: {'alpha': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.68      0.94      0.79       354\n",
      "     neutral       0.87      0.36      0.51       147\n",
      "    negative       0.75      0.51      0.61       157\n",
      "\n",
      "    accuracy                           0.71       658\n",
      "   macro avg       0.77      0.60      0.63       658\n",
      "weighted avg       0.74      0.71      0.68       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LinearSVC with Stemming Only (Sastrawi) ---\n",
      "Best Hyperparameters: {'C': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.81      0.87      0.84       354\n",
      "     neutral       0.68      0.60      0.64       147\n",
      "    negative       0.74      0.69      0.71       157\n",
      "\n",
      "    accuracy                           0.77       658\n",
      "   macro avg       0.74      0.72      0.73       658\n",
      "weighted avg       0.76      0.77      0.76       658\n",
      "\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010645 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13896\n",
      "[LightGBM] [Info] Number of data points in the train set: 2630, number of used features: 621\n",
      "[LightGBM] [Info] Start training from score -0.619854\n",
      "[LightGBM] [Info] Start training from score -1.501419\n",
      "[LightGBM] [Info] Start training from score -1.430608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tertius\\sekolah\\Sem 6\\textmining\\AOL\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LGBMClassifier with Stemming Only (Sastrawi) ---\n",
      "Best Hyperparameters: {'learning_rate': 0.05, 'n_estimators': 100, 'num_leaves': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.75      0.88      0.81       354\n",
      "     neutral       0.67      0.52      0.58       147\n",
      "    negative       0.73      0.61      0.66       157\n",
      "\n",
      "    accuracy                           0.73       658\n",
      "   macro avg       0.72      0.67      0.69       658\n",
      "weighted avg       0.73      0.73      0.73       658\n",
      "\n",
      "--- Preprocessing: Stopwords Only (Sastrawi) ---\n",
      "================================================================================\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LogisticRegression with Stopwords Only (Sastrawi) ---\n",
      "Best Hyperparameters: {'C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.80      0.84      0.82       354\n",
      "     neutral       0.66      0.56      0.61       147\n",
      "    negative       0.71      0.71      0.71       157\n",
      "\n",
      "    accuracy                           0.75       658\n",
      "   macro avg       0.72      0.71      0.71       658\n",
      "weighted avg       0.74      0.75      0.75       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- MultinomialNB with Stopwords Only (Sastrawi) ---\n",
      "Best Hyperparameters: {'alpha': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.70      0.94      0.80       354\n",
      "     neutral       0.81      0.35      0.49       147\n",
      "    negative       0.75      0.57      0.65       157\n",
      "\n",
      "    accuracy                           0.72       658\n",
      "   macro avg       0.75      0.62      0.65       658\n",
      "weighted avg       0.74      0.72      0.70       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LinearSVC with Stopwords Only (Sastrawi) ---\n",
      "Best Hyperparameters: {'C': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.79      0.85      0.82       354\n",
      "     neutral       0.67      0.56      0.61       147\n",
      "    negative       0.73      0.71      0.72       157\n",
      "\n",
      "    accuracy                           0.76       658\n",
      "   macro avg       0.73      0.71      0.72       658\n",
      "weighted avg       0.75      0.76      0.75       658\n",
      "\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9519\n",
      "[LightGBM] [Info] Number of data points in the train set: 2630, number of used features: 450\n",
      "[LightGBM] [Info] Start training from score -0.619854\n",
      "[LightGBM] [Info] Start training from score -1.501419\n",
      "[LightGBM] [Info] Start training from score -1.430608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tertius\\sekolah\\Sem 6\\textmining\\AOL\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LGBMClassifier with Stopwords Only (Sastrawi) ---\n",
      "Best Hyperparameters: {'learning_rate': 0.05, 'n_estimators': 100, 'num_leaves': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.74      0.88      0.80       354\n",
      "     neutral       0.66      0.39      0.49       147\n",
      "    negative       0.67      0.63      0.65       157\n",
      "\n",
      "    accuracy                           0.71       658\n",
      "   macro avg       0.69      0.63      0.65       658\n",
      "weighted avg       0.70      0.71      0.70       658\n",
      "\n",
      "--- Preprocessing: No Preprocessing ---\n",
      "================================================================================\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LogisticRegression with No Preprocessing ---\n",
      "Best Hyperparameters: {'C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.81      0.84      0.82       354\n",
      "     neutral       0.66      0.62      0.64       147\n",
      "    negative       0.70      0.69      0.69       157\n",
      "\n",
      "    accuracy                           0.75       658\n",
      "   macro avg       0.72      0.71      0.72       658\n",
      "weighted avg       0.75      0.75      0.75       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- MultinomialNB with No Preprocessing ---\n",
      "Best Hyperparameters: {'alpha': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.67      0.94      0.78       354\n",
      "     neutral       0.86      0.35      0.50       147\n",
      "    negative       0.72      0.48      0.57       157\n",
      "\n",
      "    accuracy                           0.70       658\n",
      "   macro avg       0.75      0.59      0.62       658\n",
      "weighted avg       0.73      0.70      0.67       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LinearSVC with No Preprocessing ---\n",
      "Best Hyperparameters: {'C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.81      0.88      0.84       354\n",
      "     neutral       0.66      0.59      0.63       147\n",
      "    negative       0.76      0.68      0.72       157\n",
      "\n",
      "    accuracy                           0.77       658\n",
      "   macro avg       0.74      0.72      0.73       658\n",
      "weighted avg       0.76      0.77      0.76       658\n",
      "\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12160\n",
      "[LightGBM] [Info] Number of data points in the train set: 2630, number of used features: 586\n",
      "[LightGBM] [Info] Start training from score -0.619854\n",
      "[LightGBM] [Info] Start training from score -1.501419\n",
      "[LightGBM] [Info] Start training from score -1.430608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tertius\\sekolah\\Sem 6\\textmining\\AOL\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LGBMClassifier with No Preprocessing ---\n",
      "Best Hyperparameters: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.71      0.86      0.78       354\n",
      "     neutral       0.61      0.46      0.52       147\n",
      "    negative       0.63      0.48      0.55       157\n",
      "\n",
      "    accuracy                           0.68       658\n",
      "   macro avg       0.65      0.60      0.61       658\n",
      "weighted avg       0.67      0.68      0.66       658\n",
      "\n",
      "--- Preprocessing: Advanced (Stem+Stop+Emoticon) ---\n",
      "================================================================================\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LogisticRegression with Advanced (Stem+Stop+Emoticon) ---\n",
      "Best Hyperparameters: {'C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.82      0.84      0.83       354\n",
      "     neutral       0.64      0.57      0.60       147\n",
      "    negative       0.72      0.76      0.74       157\n",
      "\n",
      "    accuracy                           0.76       658\n",
      "   macro avg       0.73      0.72      0.72       658\n",
      "weighted avg       0.76      0.76      0.76       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- MultinomialNB with Advanced (Stem+Stop+Emoticon) ---\n",
      "Best Hyperparameters: {'alpha': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.70      0.94      0.80       354\n",
      "     neutral       0.83      0.36      0.50       147\n",
      "    negative       0.76      0.59      0.66       157\n",
      "\n",
      "    accuracy                           0.72       658\n",
      "   macro avg       0.76      0.63      0.66       658\n",
      "weighted avg       0.74      0.72      0.70       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LinearSVC with Advanced (Stem+Stop+Emoticon) ---\n",
      "Best Hyperparameters: {'C': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.81      0.84      0.83       354\n",
      "     neutral       0.66      0.56      0.61       147\n",
      "    negative       0.71      0.75      0.73       157\n",
      "\n",
      "    accuracy                           0.76       658\n",
      "   macro avg       0.73      0.72      0.72       658\n",
      "weighted avg       0.75      0.76      0.75       658\n",
      "\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9367\n",
      "[LightGBM] [Info] Number of data points in the train set: 2630, number of used features: 430\n",
      "[LightGBM] [Info] Start training from score -0.619854\n",
      "[LightGBM] [Info] Start training from score -1.501419\n",
      "[LightGBM] [Info] Start training from score -1.430608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tertius\\sekolah\\Sem 6\\textmining\\AOL\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LGBMClassifier with Advanced (Stem+Stop+Emoticon) ---\n",
      "Best Hyperparameters: {'learning_rate': 0.05, 'n_estimators': 100, 'num_leaves': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.75      0.87      0.81       354\n",
      "     neutral       0.65      0.46      0.54       147\n",
      "    negative       0.70      0.64      0.67       157\n",
      "\n",
      "    accuracy                           0.72       658\n",
      "   macro avg       0.70      0.66      0.67       658\n",
      "weighted avg       0.72      0.72      0.71       658\n",
      "\n",
      "--- Preprocessing: Advanced (Stop+Emoticon) ---\n",
      "================================================================================\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LogisticRegression with Advanced (Stop+Emoticon) ---\n",
      "Best Hyperparameters: {'C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.81      0.84      0.83       354\n",
      "     neutral       0.65      0.59      0.62       147\n",
      "    negative       0.73      0.73      0.73       157\n",
      "\n",
      "    accuracy                           0.76       658\n",
      "   macro avg       0.73      0.72      0.73       658\n",
      "weighted avg       0.76      0.76      0.76       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- MultinomialNB with Advanced (Stop+Emoticon) ---\n",
      "Best Hyperparameters: {'alpha': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.71      0.92      0.80       354\n",
      "     neutral       0.78      0.36      0.49       147\n",
      "    negative       0.72      0.59      0.65       157\n",
      "\n",
      "    accuracy                           0.72       658\n",
      "   macro avg       0.74      0.62      0.65       658\n",
      "weighted avg       0.73      0.72      0.70       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LinearSVC with Advanced (Stop+Emoticon) ---\n",
      "Best Hyperparameters: {'C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.79      0.85      0.82       354\n",
      "     neutral       0.65      0.57      0.61       147\n",
      "    negative       0.77      0.71      0.74       157\n",
      "\n",
      "    accuracy                           0.76       658\n",
      "   macro avg       0.73      0.71      0.72       658\n",
      "weighted avg       0.75      0.76      0.75       658\n",
      "\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8726\n",
      "[LightGBM] [Info] Number of data points in the train set: 2630, number of used features: 401\n",
      "[LightGBM] [Info] Start training from score -0.619854\n",
      "[LightGBM] [Info] Start training from score -1.501419\n",
      "[LightGBM] [Info] Start training from score -1.430608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tertius\\sekolah\\Sem 6\\textmining\\AOL\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LGBMClassifier with Advanced (Stop+Emoticon) ---\n",
      "Best Hyperparameters: {'learning_rate': 0.05, 'n_estimators': 200, 'num_leaves': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.74      0.86      0.80       354\n",
      "     neutral       0.57      0.41      0.48       147\n",
      "    negative       0.68      0.60      0.64       157\n",
      "\n",
      "    accuracy                           0.70       658\n",
      "   macro avg       0.66      0.63      0.64       658\n",
      "weighted avg       0.69      0.70      0.69       658\n",
      "\n",
      "--- Preprocessing: Advanced (Stem+Emoticon) ---\n",
      "================================================================================\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LogisticRegression with Advanced (Stem+Emoticon) ---\n",
      "Best Hyperparameters: {'C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.83      0.85      0.84       354\n",
      "     neutral       0.66      0.63      0.64       147\n",
      "    negative       0.74      0.74      0.74       157\n",
      "\n",
      "    accuracy                           0.77       658\n",
      "   macro avg       0.74      0.74      0.74       658\n",
      "weighted avg       0.77      0.77      0.77       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- MultinomialNB with Advanced (Stem+Emoticon) ---\n",
      "Best Hyperparameters: {'alpha': 0.1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.69      0.94      0.79       354\n",
      "     neutral       0.86      0.37      0.51       147\n",
      "    negative       0.76      0.54      0.63       157\n",
      "\n",
      "    accuracy                           0.72       658\n",
      "   macro avg       0.77      0.61      0.65       658\n",
      "weighted avg       0.74      0.72      0.69       658\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "\n",
      "--- LinearSVC with Advanced (Stem+Emoticon) ---\n",
      "Best Hyperparameters: {'C': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.82      0.87      0.84       354\n",
      "     neutral       0.68      0.61      0.64       147\n",
      "    negative       0.74      0.73      0.73       157\n",
      "\n",
      "    accuracy                           0.78       658\n",
      "   macro avg       0.75      0.73      0.74       658\n",
      "weighted avg       0.77      0.78      0.77       658\n",
      "\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13254\n",
      "[LightGBM] [Info] Number of data points in the train set: 2630, number of used features: 578\n",
      "[LightGBM] [Info] Start training from score -0.619854\n",
      "[LightGBM] [Info] Start training from score -1.501419\n",
      "[LightGBM] [Info] Start training from score -1.430608\n",
      "\n",
      "--- LGBMClassifier with Advanced (Stem+Emoticon) ---\n",
      "Best Hyperparameters: {'learning_rate': 0.05, 'n_estimators': 100, 'num_leaves': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.88      0.82       354\n",
      "     neutral       0.65      0.50      0.57       147\n",
      "    negative       0.72      0.62      0.67       157\n",
      "\n",
      "    accuracy                           0.74       658\n",
      "   macro avg       0.71      0.67      0.68       658\n",
      "weighted avg       0.73      0.74      0.73       658\n",
      "\n",
      "\n",
      "--- Final Results with Hyperparameter Tuning ---\n",
      "                 Model                    Preprocessing  \\\n",
      "26           LinearSVC         Advanced (Stem+Emoticon)   \n",
      "4   LogisticRegression         Stemming Only (Sastrawi)   \n",
      "24  LogisticRegression         Advanced (Stem+Emoticon)   \n",
      "14           LinearSVC                 No Preprocessing   \n",
      "6            LinearSVC         Stemming Only (Sastrawi)   \n",
      "20  LogisticRegression         Advanced (Stop+Emoticon)   \n",
      "16  LogisticRegression    Advanced (Stem+Stop+Emoticon)   \n",
      "0   LogisticRegression  Stemming + Stopwords (Sastrawi)   \n",
      "2            LinearSVC  Stemming + Stopwords (Sastrawi)   \n",
      "18           LinearSVC    Advanced (Stem+Stop+Emoticon)   \n",
      "22           LinearSVC         Advanced (Stop+Emoticon)   \n",
      "10           LinearSVC        Stopwords Only (Sastrawi)   \n",
      "12  LogisticRegression                 No Preprocessing   \n",
      "8   LogisticRegression        Stopwords Only (Sastrawi)   \n",
      "3       LGBMClassifier  Stemming + Stopwords (Sastrawi)   \n",
      "27      LGBMClassifier         Advanced (Stem+Emoticon)   \n",
      "7       LGBMClassifier         Stemming Only (Sastrawi)   \n",
      "1        MultinomialNB  Stemming + Stopwords (Sastrawi)   \n",
      "17       MultinomialNB    Advanced (Stem+Stop+Emoticon)   \n",
      "19      LGBMClassifier    Advanced (Stem+Stop+Emoticon)   \n",
      "9        MultinomialNB        Stopwords Only (Sastrawi)   \n",
      "21       MultinomialNB         Advanced (Stop+Emoticon)   \n",
      "25       MultinomialNB         Advanced (Stem+Emoticon)   \n",
      "11      LGBMClassifier        Stopwords Only (Sastrawi)   \n",
      "5        MultinomialNB         Stemming Only (Sastrawi)   \n",
      "23      LGBMClassifier         Advanced (Stop+Emoticon)   \n",
      "13       MultinomialNB                 No Preprocessing   \n",
      "15      LGBMClassifier                 No Preprocessing   \n",
      "\n",
      "                                          Best Params  Accuracy  \\\n",
      "26                                           {'C': 1}  0.775076   \n",
      "4                                           {'C': 10}  0.775076   \n",
      "24                                          {'C': 10}  0.773556   \n",
      "14                                          {'C': 10}  0.768997   \n",
      "6                                            {'C': 1}  0.765957   \n",
      "20                                          {'C': 10}  0.759878   \n",
      "16                                          {'C': 10}  0.758359   \n",
      "0                                           {'C': 10}  0.758359   \n",
      "2                                           {'C': 10}  0.758359   \n",
      "18                                           {'C': 1}  0.756839   \n",
      "22                                          {'C': 10}  0.755319   \n",
      "10                                           {'C': 1}  0.755319   \n",
      "12                                          {'C': 10}  0.752280   \n",
      "8                                           {'C': 10}  0.749240   \n",
      "3   {'learning_rate': 0.05, 'n_estimators': 100, '...  0.738602   \n",
      "27  {'learning_rate': 0.05, 'n_estimators': 100, '...  0.735562   \n",
      "7   {'learning_rate': 0.05, 'n_estimators': 100, '...  0.734043   \n",
      "1                                      {'alpha': 0.1}  0.724924   \n",
      "17                                     {'alpha': 0.1}  0.724924   \n",
      "19  {'learning_rate': 0.05, 'n_estimators': 100, '...  0.724924   \n",
      "9                                      {'alpha': 0.1}  0.720365   \n",
      "21                                     {'alpha': 0.1}  0.717325   \n",
      "25                                     {'alpha': 0.1}  0.715805   \n",
      "11  {'learning_rate': 0.05, 'n_estimators': 100, '...  0.711246   \n",
      "5                                      {'alpha': 0.1}  0.708207   \n",
      "23  {'learning_rate': 0.05, 'n_estimators': 200, '...  0.700608   \n",
      "13                                     {'alpha': 0.1}  0.697568   \n",
      "15  {'learning_rate': 0.1, 'n_estimators': 100, 'n...  0.677812   \n",
      "\n",
      "    F1-Score (Weighted)  \n",
      "26             0.772231  \n",
      "4              0.773407  \n",
      "24             0.772356  \n",
      "14             0.764798  \n",
      "6              0.762527  \n",
      "20             0.757769  \n",
      "16             0.756187  \n",
      "0              0.754496  \n",
      "2              0.755534  \n",
      "18             0.753637  \n",
      "22             0.751930  \n",
      "10             0.751267  \n",
      "12             0.750704  \n",
      "8              0.745582  \n",
      "3              0.727981  \n",
      "27             0.726854  \n",
      "7              0.725410  \n",
      "1              0.702195  \n",
      "17             0.702431  \n",
      "19             0.714484  \n",
      "9              0.696597  \n",
      "21             0.695389  \n",
      "25             0.692086  \n",
      "11             0.695790  \n",
      "5              0.682988  \n",
      "23             0.688635  \n",
      "13             0.669776  \n",
      "15             0.664386  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tertius\\sekolah\\Sem 6\\textmining\\AOL\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Define Models with Hyperparameter Grids and Enhancements ---\n",
    "models_and_params = [\n",
    "    {\n",
    "        'model': LogisticRegression(\n",
    "            max_iter=2000, \n",
    "            class_weight='balanced' # Handles class imbalance\n",
    "        ),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model': MultinomialNB(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 0.5, 1.0]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model': LinearSVC(\n",
    "            max_iter=5000, \n",
    "            class_weight='balanced', # Handles class imbalance\n",
    "            dual=True \n",
    "        ),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model': lgb.LGBMClassifier(\n",
    "            objective='multiclass', \n",
    "            random_state=42\n",
    "        ),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'num_leaves': [20, 31]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- Run the Main Experiment Loop and Print Results ---\n",
    "results = []\n",
    "\n",
    "for pp_name, pp_func in preprocessing_options.items():\n",
    "    print(f'--- Preprocessing: {pp_name} ---')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Apply the preprocessing with N-grams\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        preprocessor=pp_func,\n",
    "        ngram_range=(1, 2) # Use both unigrams and bigrams\n",
    "    )\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    for model_info in models_and_params:\n",
    "        model = model_info['model']\n",
    "        params = model_info['params']\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "        # Set up and run GridSearchCV\n",
    "        grid_search = GridSearchCV(model, params, cv=5, n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(X_train_tfidf, y_train)\n",
    "        \n",
    "        # Get the best model and evaluate it\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test_tfidf)\n",
    "        report = classification_report(y_test, y_pred, target_names=label_map.keys(), output_dict=True, zero_division=0)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Preprocessing': pp_name,\n",
    "            'Best Params': grid_search.best_params_,\n",
    "            'Accuracy': report['accuracy'],\n",
    "            'F1-Score (Weighted)': report['weighted avg']['f1-score']\n",
    "        })\n",
    "        \n",
    "        # Print the intermediate report for each combination\n",
    "        print(f'\\n--- {model_name} with {pp_name} ---')\n",
    "        print(f'Best Hyperparameters: {grid_search.best_params_}')\n",
    "        print(classification_report(y_test, y_pred, target_names=label_map.keys(), zero_division=0))\n",
    "\n",
    "# --- Display Final Comparative Results Table ---\n",
    "results_df_tfidf = pd.DataFrame(results)\n",
    "print(\"\\n--- Final Results with Hyperparameter Tuning ---\")\n",
    "# Display relevant columns for clarity\n",
    "display_cols = ['Model', 'Preprocessing', 'Best Params', 'Accuracy', 'F1-Score (Weighted)']\n",
    "print(results_df_tfidf[display_cols].sort_values(by='Accuracy', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
